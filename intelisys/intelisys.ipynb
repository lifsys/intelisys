{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Provides intelligence/AI services for the Lifsys Enterprise.\n",
    "\n",
    "This module requires a 1Password Connect server to be available and configured.\n",
    "The OP_CONNECT_TOKEN and OP_CONNECT_HOST environment variables must be set\n",
    "for the onepasswordconnectsdk to function properly.\n",
    "\n",
    "Example usage for image OCR:\n",
    "    intelisys = Intelisys(provider=\"openrouter\", model=\"google/gemini-pro-vision\")  # Make sure to use a model that supports image processing\n",
    "    result = (intelisys\n",
    "        .image(\"https://mintlify.s3-us-west-1.amazonaws.com/anthropic/images/how-to-prompt-eng.png\")\n",
    "        .chat(\"Please provide the complete text in the following image(s).\")\n",
    "    )\n",
    "    result\n",
    "\"\"\"\n",
    "import re\n",
    "import ast\n",
    "import json\n",
    "import os\n",
    "import base64\n",
    "import io\n",
    "import requests\n",
    "from typing import Dict, Optional, Union, Tuple, Any, Type\n",
    "from contextlib import contextmanager\n",
    "from PIL import Image\n",
    "from anthropic import Anthropic, AsyncAnthropic\n",
    "from jinja2 import Template\n",
    "from openai import AsyncOpenAI, OpenAI\n",
    "from termcolor import colored\n",
    "import logging\n",
    "from pydantic import BaseModel, ValidationError\n",
    "from functools import lru_cache\n",
    "\n",
    "# Define the log format\n",
    "DATETIME_FORMAT = \"%Y-%m-%d %H:%M:%S\"\n",
    "LOG_FORMAT = \"%(asctime)s | %(levelname)s | %(message)s\"\n",
    "\n",
    "# Configure the root logger\n",
    "logging.basicConfig(level=logging.WARNING, datefmt=DATETIME_FORMAT, format=LOG_FORMAT, force=True)\n",
    "\n",
    "# Create a logger for this module\n",
    "logger = logging.getLogger(\"Global\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "def remove_preface(text: str) -> str:\n",
    "    \"\"\"Remove any prefaced text before the start of JSON content.\"\"\"\n",
    "    match: Optional[re.Match] = re.search(r\"[\\{\\[]\", text)\n",
    "    if match:\n",
    "        start: int = match.start()\n",
    "        return text[start:]\n",
    "    return text\n",
    "\n",
    "def locate_json_error(json_str: str, error_msg: str) -> Tuple[int, int, str]:\n",
    "    \"\"\"Locate the position of the JSON error and return the surrounding context.\"\"\"\n",
    "    match = re.search(r\"line (\\d+) column (\\d+)\", error_msg)\n",
    "    if not match:\n",
    "        return 0, 0, \"Could not parse error message\"\n",
    "    line_no, col_no = map(int, match.groups())\n",
    "    lines = json_str.splitlines()\n",
    "    if line_no > len(lines):\n",
    "        return line_no, col_no, \"Line number exceeds total lines in JSON string\"\n",
    "    problematic_line = lines[line_no - 1]\n",
    "    start, end = max(0, col_no - 20), min(len(problematic_line), col_no + 20)\n",
    "    context = problematic_line[start:end]\n",
    "    pointer = f\"{' ' * min(20, col_no - 1)}^\"\n",
    "    return line_no, col_no, f\"{context}\\n{pointer}\"\n",
    "\n",
    "def iterative_llm_fix_json(json_string: str, max_attempts: int = 5, intelisys_instance=None) -> str:\n",
    "    logger.info(f\"Starting iterative_llm_fix_json with input: {json_string}\")\n",
    "    if intelisys_instance is None:\n",
    "        intelisys_instance = Intelisys(provider=\"openai\", model=\"gpt-3.5-turbo\", api_key=\"dummy_key\")\n",
    "    attempts = 0\n",
    "    \n",
    "    while attempts < max_attempts:\n",
    "        prompt = f\"Fix this JSON: {json_string}\"\n",
    "        logger.debug(f\"Sending prompt to AI: {prompt}\")\n",
    "        response = intelisys_instance.chat(prompt)\n",
    "        logger.debug(f\"Received response from AI: {response}\")\n",
    "        \n",
    "        try:\n",
    "            json.loads(response)  # Try to parse the AI's response\n",
    "            logger.info(f\"Successfully parsed JSON on attempt {attempts + 1}\")\n",
    "            return response  # If successful, return the AI's response\n",
    "        except json.JSONDecodeError:\n",
    "            logger.warning(f\"JSON parsing failed on attempt {attempts + 1}\")\n",
    "            json_string = response  # Update json_string with the AI's response for the next attempt\n",
    "        \n",
    "        attempts += 1\n",
    "    \n",
    "    logger.warning(f\"Reached max attempts. Returning: {json_string}\")\n",
    "    return json_string  # Return the last attempt even if it's not valid JSON\n",
    "\n",
    "def safe_json_loads(json_str: str, error_prefix: str = \"\") -> Dict:\n",
    "    \"\"\"Safely convert any string input into JSON, with iterative LLM-based error correction.\"\"\"\n",
    "    if json_str is None:\n",
    "        raise ValueError(f\"{error_prefix}Input is None\")\n",
    "\n",
    "    if not isinstance(json_str, str):\n",
    "        raise TypeError(f\"{error_prefix}Input must be a string, not {type(json_str)}\")\n",
    "\n",
    "    json_str = remove_preface(json_str)\n",
    "    \n",
    "    fix_attempts = [\n",
    "        json.loads,\n",
    "        lambda s: Intelisys(\n",
    "            provider=\"openai\", \n",
    "            model=\"gpt-4o-mini\",\n",
    "            json_mode=True) \\\n",
    "            .set_system_message(\"Convert the following text into valid JSON. If it's already valid JSON, return it as is.\") \\\n",
    "            .chat(f\"Convert this to JSON:\\n{s}\"),\n",
    "        iterative_llm_fix_json,\n",
    "        lambda s: ast.literal_eval(s) if s.strip().startswith('{') else {\"content\": s}\n",
    "    ]\n",
    "    \n",
    "    for fix in fix_attempts:\n",
    "        try:\n",
    "            fixed_json = fix(json_str)\n",
    "            if isinstance(fixed_json, dict):\n",
    "                return fixed_json\n",
    "            elif isinstance(fixed_json, str):\n",
    "                # If it's still a string, try to parse it as JSON one more time\n",
    "                return json.loads(fixed_json)\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"{error_prefix}JSON conversion attempt failed: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # If all attempts fail, create a simple JSON object with the original string as content\n",
    "    logger.warning(f\"{error_prefix}Failed to convert to JSON. Creating a simple JSON object.\")\n",
    "    return {\"content\": json_str}\n",
    "\n",
    "class Intelisys:\n",
    "    \"\"\"\n",
    "    A class for interacting with various AI providers and models.\n",
    "\n",
    "    This class provides a unified interface for chatting with AI models,\n",
    "    handling image inputs, and managing conversation history.\n",
    "\n",
    "    Attributes:\n",
    "        SUPPORTED_PROVIDERS (set): Set of supported AI providers.\n",
    "        DEFAULT_MODELS (dict): Default models for each provider.\n",
    "\n",
    "    Args:\n",
    "        name (str): Name of the Intelisys instance.\n",
    "        api_key (str, optional): API key for the chosen provider.\n",
    "        max_history_words (int): Maximum number of words to keep in conversation history.\n",
    "        max_words_per_message (int, optional): Maximum words per message.\n",
    "        json_mode (bool): Whether to return responses in JSON format.\n",
    "        stream (bool): Whether to stream the response.\n",
    "        use_async (bool): Whether to use async methods.\n",
    "        max_retry (int): Maximum number of retries for API calls.\n",
    "        provider (str): AI provider to use (e.g., \"openai\", \"anthropic\").\n",
    "        model (str, optional): Specific model to use.\n",
    "        should_print_init (bool): Whether to print initialization details.\n",
    "        print_color (str): Color for printed output.\n",
    "        temperature (float): Temperature for response generation.\n",
    "        max_tokens (int, optional): Maximum tokens for response.\n",
    "        log (str or int): Logging level.\n",
    "\n",
    "    Usage:\n",
    "        intelisys = Intelisys(provider=\"openai\", model=\"gpt-4\")\n",
    "        response = intelisys.chat(\"Hello, how are you?\")\n",
    "    \"\"\"\n",
    "    # Define the log format\n",
    "    DATETIME_FORMAT = \"%Y-%m-%d %H:%M:%S\"\n",
    "    LOG_FORMAT = \"%(asctime)s | %(levelname)s | %(message)s\"\n",
    " \n",
    "    SUPPORTED_PROVIDERS = {\"openai\", \"anthropic\", \"openrouter\", \"groq\"}\n",
    "    DEFAULT_MODELS = {\n",
    "        \"openai\": \"gpt-4o-2024-08-06\",\n",
    "        \"anthropic\": \"claude-3-5-sonnet-20240620\",\n",
    "        \"openrouter\": \"meta-llama/llama-3.1-405b-instruct\",\n",
    "        \"groq\": \"llama-3.1-8b-instant\"\n",
    "    }\n",
    "\n",
    "    def __init__(self, name=\"Intelisys\", api_key=None, max_history_words=0,\n",
    "                 max_words_per_message=None, json_mode=False, stream=False, use_async=False,\n",
    "                 max_retry=10, provider=\"anthropic\", model=None, should_print_init=False,\n",
    "                 print_color=\"green\", temperature=0, max_tokens=None, log: Union[str, int] = \"WARNING\"):\n",
    "        \"\"\"\n",
    "        Initialize the Intelisys instance.\n",
    "\n",
    "        Args:\n",
    "            name (str): Name of the Intelisys instance.\n",
    "            api_key (str, optional): API key for the chosen provider.\n",
    "            max_history_words (int): Maximum number of words to keep in conversation history.\n",
    "            max_words_per_message (int, optional): Maximum words per message.\n",
    "            json_mode (bool): Whether to return responses in JSON format.\n",
    "            stream (bool): Whether to stream the response.\n",
    "            use_async (bool): Whether to use async methods.\n",
    "            max_retry (int): Maximum number of retries for API calls.\n",
    "            provider (str): AI provider to use (e.g., \"openai\", \"anthropic\").\n",
    "            model (str, optional): Specific model to use.\n",
    "            should_print_init (bool): Whether to print initialization details.\n",
    "            print_color (str): Color for printed output.\n",
    "            temperature (float): Temperature for response generation.\n",
    "            max_tokens (int, optional): Maximum tokens for response.\n",
    "            log (str or int): Logging level.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Set up logger\n",
    "        self.logger = logging.getLogger(f\"{self.__class__.__name__}.{name}\")\n",
    "        self.set_log_level(log)\n",
    "        \n",
    "        self.logger.debug(f\"Initializing Intelisys instance '{name}' with provider={provider}, model={model}\")\n",
    "        \n",
    "        self.provider = provider.lower()\n",
    "        if self.provider not in self.SUPPORTED_PROVIDERS:\n",
    "            self._raise_unsupported_provider_error()\n",
    "        \n",
    "        self.name = name\n",
    "        self._api_key = api_key\n",
    "        self.temperature = temperature\n",
    "        self.history = []\n",
    "        self.max_history_words = max_history_words\n",
    "        self.max_words_per_message = max_words_per_message\n",
    "        self.json_mode = json_mode\n",
    "        if self.json_mode and self.provider != \"openai\":\n",
    "            self.logger.debug(f\"json_mode=True is set for provider '{self.provider}'\")\n",
    "        self.stream = stream\n",
    "        self.use_async = use_async\n",
    "        self.max_retry = max_retry\n",
    "        self.print_color = print_color\n",
    "        self.max_tokens = max_tokens\n",
    "        self.system_message = \"You are a helpful assistant.\"\n",
    "        if self.provider == \"openai\" and self.json_mode:\n",
    "            self.system_message += \" Please return your response in JSON\"\n",
    "\n",
    "        self._model = model or self.DEFAULT_MODELS.get(self.provider)\n",
    "        self._client = None\n",
    "        self.last_response = None\n",
    "\n",
    "        self.default_template = \"{{ prompt }}\"\n",
    "        self.default_persona = \"You are a helpful assistant.\"\n",
    "        self.template_instruction = \"\"\n",
    "        self.template_persona = \"\"\n",
    "        self.template_data = {}\n",
    "        self.image_urls = []\n",
    "        self.current_message = None\n",
    "        \n",
    "        if should_print_init:\n",
    "            print(colored(f\"\\n{self.name} initialized with provider={self.provider}, model={self.model}, json_mode={self.json_mode}, temp={self.temperature}\", \"red\"))\n",
    "\n",
    "        self.logger.debug(f\"Intelisys initialized with: {', '.join(f'{k}={v}' for k, v in locals().items() if k != 'self')}\")\n",
    "\n",
    "        self.output_model = None\n",
    "        self.structured_output = None\n",
    "\n",
    "    def set_log_level(self, level: Union[int, str]):\n",
    "        if isinstance(level, str):\n",
    "            level = level.upper()\n",
    "            if not hasattr(logging, level):\n",
    "                raise ValueError(f\"Invalid log level: {level}\")\n",
    "            level = getattr(logging, level)\n",
    "        \n",
    "        self.logger.setLevel(level)\n",
    "        \n",
    "        # Remove all existing handlers\n",
    "        for handler in self.logger.handlers[:]:\n",
    "            self.logger.removeHandler(handler)\n",
    "        \n",
    "        # Add a single handler with the correct formatter\n",
    "        handler = logging.StreamHandler()\n",
    "        formatter = logging.Formatter(self.LOG_FORMAT, datefmt=self.DATETIME_FORMAT)\n",
    "        handler.setFormatter(formatter)\n",
    "        self.logger.addHandler(handler)\n",
    "        \n",
    "        # Prevent the logger from propagating messages to the root logger\n",
    "        self.logger.propagate = False\n",
    "        \n",
    "        self.logger.debug(\"Log level set to: %s\", logging.getLevelName(level))\n",
    "\n",
    "    def _raise_unsupported_provider_error(self):\n",
    "        import difflib\n",
    "        close_matches = difflib.get_close_matches(self.provider, self.SUPPORTED_PROVIDERS, n=1, cutoff=0.6)\n",
    "        suggestion = f\"Did you mean '{close_matches[0]}'?\" if close_matches else \"Please check the spelling and try again.\"\n",
    "        raise ValueError(f\"Unsupported provider: '{self.provider}'. {suggestion}\\nSupported providers are: {', '.join(self.SUPPORTED_PROVIDERS)}\")\n",
    "\n",
    "    @property\n",
    "    def model(self):\n",
    "        return self._model or self.DEFAULT_MODELS.get(self.provider)\n",
    "\n",
    "    @property\n",
    "    def api_key(self):\n",
    "        return self._api_key or self._get_api_key()\n",
    "\n",
    "    @property\n",
    "    def client(self):\n",
    "        if self._client is None:\n",
    "            self._initialize_client()\n",
    "        return self._client\n",
    "\n",
    "    @staticmethod\n",
    "    @lru_cache(maxsize=128)\n",
    "    def _go_get_api(item: str, key_name: str, vault: str = \"API\") -> str:\n",
    "        try:\n",
    "            from onepasswordconnectsdk import new_client_from_environment\n",
    "            client = new_client_from_environment()\n",
    "            item = client.get_item(item, vault)\n",
    "            for field in item.fields:\n",
    "                if field.label == key_name:\n",
    "                    return field.value\n",
    "            raise ValueError(f\"Key '{key_name}' not found in item '{item}'\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"1Password Connect Error: {e}\")\n",
    "        \n",
    "    def _get_api_key(self):\n",
    "        env_var_map = {\n",
    "            \"openai\": \"OPENAI_API_KEY\",\n",
    "            \"anthropic\": \"ANTHROPIC_API_KEY\",\n",
    "            \"openrouter\": \"OPENROUTER_API_KEY\",\n",
    "            \"groq\": \"GROQ_API_KEY\"\n",
    "        }\n",
    "        item_map = {\n",
    "            \"openai\": (\"OPEN-AI\", \"Cursor\"),\n",
    "            \"anthropic\": (\"Anthropic\", \"Cursor\"),\n",
    "            \"openrouter\": (\"OpenRouter\", \"Cursor\"),\n",
    "            \"groq\": (\"Groq\", \"Promptsys\")\n",
    "        }\n",
    "        \n",
    "        env_var = env_var_map.get(self.provider)\n",
    "        item, key = item_map.get(self.provider, (None, None))\n",
    "        \n",
    "        if env_var and item and key:\n",
    "            return os.getenv(env_var) or self._go_get_api(item, key)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported provider: {self.provider}\")\n",
    "\n",
    "    def _initialize_client(self):\n",
    "        self.logger.debug(f\"Initializing client for provider: {self.provider}\")\n",
    "        if self.use_async:\n",
    "            if self.provider == \"anthropic\":\n",
    "                self._client = AsyncAnthropic(api_key=self.api_key)\n",
    "            else:\n",
    "                base_url = \"https://api.groq.com/openai/v1\" if self.provider == \"groq\" else \"https://openrouter.ai/api/v1\" if self.provider == \"openrouter\" else None\n",
    "                self._client = AsyncOpenAI(base_url=base_url, api_key=self.api_key)\n",
    "        else:\n",
    "            if self.provider == \"anthropic\":\n",
    "                self._client = Anthropic(api_key=self.api_key)\n",
    "            else:\n",
    "                base_url = \"https://api.groq.com/openai/v1\" if self.provider == \"groq\" else \"https://openrouter.ai/api/v1\" if self.provider == \"openrouter\" else None\n",
    "                self._client = OpenAI(base_url=base_url, api_key=self.api_key)\n",
    "        self.logger.debug(f\"Client initialized: {type(self._client).__name__}\")\n",
    "\n",
    "    def set_system_message(self, message=None):\n",
    "        \"\"\"\n",
    "        Set the system message for the conversation.\n",
    "\n",
    "        Args:\n",
    "            message (str, optional): The system message to set. If None, a default message is used.\n",
    "\n",
    "        Returns:\n",
    "            self: The Intelisys instance for method chaining.\n",
    "\n",
    "        Usage:\n",
    "            intelisys.set_system_message(\"You are a helpful assistant specialized in Python programming.\")\n",
    "        \"\"\"\n",
    "        self.system_message = message or \"You are a helpful assistant.\"\n",
    "        if self.provider == \"openai\" and self.json_mode and \"json\" not in message.lower():\n",
    "            self.system_message += \" Please return your response in JSON unless user has specified a system message.\"\n",
    "        self.logger.debug(f\"System message set: {self.system_message[:50]}...\")  # Log first 50 chars\n",
    "        return self\n",
    "\n",
    "    def chat(self, user_input):\n",
    "        \"\"\"\n",
    "        Send a chat message to the AI and return the response.\n",
    "\n",
    "        Args:\n",
    "            user_input (str): The user's message to send to the AI.\n",
    "\n",
    "        Returns:\n",
    "            Union[str, BaseModel]: The AI's response as a string or a Pydantic model instance if structured output is used.\n",
    "\n",
    "        Usage:\n",
    "            response = intelisys.chat(\"What is the capital of France?\")\n",
    "        \"\"\"\n",
    "        self.logger.debug(\"*Chat*\")\n",
    "        self.logger.debug(f\"User input: {user_input[:50]}...\")\n",
    "        self.current_message = {\"role\": \"user\", \"content\": user_input}\n",
    "        if self.max_history_words > 0:\n",
    "            self.add_message(\"user\", user_input)\n",
    "        \n",
    "        try:\n",
    "            response = self._create_response(self.max_tokens or (4000 if self.provider != \"anthropic\" else 8192))\n",
    "            self.logger.debug(f\"Raw API response: {response}\")\n",
    "            result = self._handle_response(response)\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in chat method: {str(e)}\")\n",
    "            raise\n",
    "        \n",
    "        self.current_message = None\n",
    "        self.image_urls = []  # Clear image URLs after sending\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def _encode_image(self, image_path: str) -> str:\n",
    "        self.logger.debug(f\"Encoding image: {image_path}\")\n",
    "        with Image.open(image_path) as img:\n",
    "            if img.mode != 'RGB':\n",
    "                img = img.convert('RGB')\n",
    "            byte_arr = io.BytesIO()\n",
    "            img.save(byte_arr, format='PNG')\n",
    "            return base64.b64encode(byte_arr.getvalue()).decode('utf-8')\n",
    "\n",
    "    def image(self, path_or_url: str, detail: str = \"auto\"):\n",
    "        \"\"\"\n",
    "        Add an image to the current message for image-based AI tasks.\n",
    "\n",
    "        Args:\n",
    "            path_or_url (str): Local file path or URL of the image.\n",
    "            detail (str, optional): Level of detail for image analysis (default is \"auto\").\n",
    "\n",
    "        Returns:\n",
    "            self: The Intelisys instance for method chaining.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the provider doesn't support image inputs.\n",
    "            FileNotFoundError: If the local image file is not found.\n",
    "\n",
    "        Usage:\n",
    "            intelisys.chat(\"Describe this image\").image(\"/path/to/image.jpg\").get_response()\n",
    "        \"\"\"\n",
    "        self.logger.debug(f\"Image method called with path_or_url: {path_or_url}\")\n",
    "        if self.provider not in [\"openai\", \"openrouter\"]:\n",
    "            raise ValueError(\"The image method is only supported for the OpenAI and OpenRouter providers.\")\n",
    "        \n",
    "        if path_or_url.startswith(('http://', 'https://')):\n",
    "            response = requests.get(path_or_url)\n",
    "            response.raise_for_status()\n",
    "            image_data = base64.b64encode(response.content).decode('utf-8')\n",
    "        else:\n",
    "            # Validate local file path\n",
    "            if not os.path.exists(path_or_url):\n",
    "                raise FileNotFoundError(f\"Image file not found: {path_or_url}\")\n",
    "            with open(path_or_url, \"rb\") as image_file:\n",
    "                image_data = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "        self.image_urls.append(f\"data:image/jpeg;base64,{image_data}\")\n",
    "        self.logger.debug(f\"Added image: {path_or_url}\")\n",
    "        return self\n",
    "\n",
    "    def _create_response(self, max_tokens, **kwargs):\n",
    "        messages = self.history.copy() if self.max_history_words > 0 else [self.current_message]\n",
    "        \n",
    "        common_params = {\n",
    "            \"model\": self.model,\n",
    "            \"messages\": messages,\n",
    "            \"stream\": self.stream,\n",
    "            \"temperature\": self.temperature,\n",
    "            **kwargs\n",
    "        }\n",
    "\n",
    "        if self.provider == \"anthropic\":\n",
    "            return self._create_anthropic_response(common_params, max_tokens)\n",
    "        else:\n",
    "            return self._create_other_provider_response(common_params, max_tokens)\n",
    "\n",
    "    def _create_anthropic_response(self, common_params, max_tokens):\n",
    "        anthropic_max_tokens = min(max_tokens, 4096)\n",
    "        extra_headers = {\"anthropic-beta\": \"max-tokens-3-5-sonnet-2024-07-15\"}\n",
    "        \n",
    "        return self.client.messages.create(\n",
    "            system=self.system_message,\n",
    "            max_tokens=anthropic_max_tokens,\n",
    "            extra_headers=extra_headers,\n",
    "            **common_params\n",
    "        )\n",
    "\n",
    "    def _create_other_provider_response(self, common_params, max_tokens):\n",
    "        if max_tokens:\n",
    "            common_params[\"max_tokens\"] = max_tokens\n",
    "\n",
    "        if self.image_urls and self.provider in [\"openai\", \"openrouter\"]:\n",
    "            self._add_image_content(common_params)\n",
    "\n",
    "        if self.json_mode and self.provider == \"openai\":\n",
    "            common_params[\"response_format\"] = {\"type\": \"json_object\"}\n",
    "        \n",
    "        if self.system_message:\n",
    "            common_params[\"messages\"].insert(0, {\"role\": \"system\", \"content\": self.system_message})\n",
    "\n",
    "        if self.provider == \"openai\" and self.output_model:\n",
    "            self._add_output_model_params(common_params)\n",
    "\n",
    "        self.logger.debug(f\"API call params: {common_params}\")\n",
    "        return self.client.chat.completions.create(**common_params)\n",
    "\n",
    "    def _add_image_content(self, common_params):\n",
    "        last_message = common_params[\"messages\"][-1]\n",
    "        content = [{\"type\": \"text\", \"text\": last_message[\"content\"]}] if isinstance(last_message[\"content\"], str) else []\n",
    "        content.extend({\"type\": \"image_url\", \"image_url\": {\"url\": url}} for url in self.image_urls)\n",
    "        last_message[\"content\"] = content\n",
    "\n",
    "    def _add_output_model_params(self, common_params):\n",
    "        common_params[\"response_format\"] = {\"type\": \"json_object\"}\n",
    "        common_params[\"functions\"] = [{\n",
    "            \"name\": \"output\",\n",
    "            \"parameters\": self.output_model.model_json_schema()\n",
    "        }]\n",
    "        common_params[\"function_call\"] = {\"name\": \"output\"}\n",
    "        \n",
    "        json_instruction = \"Please return your response in JSON format according to the specified schema.\"\n",
    "        if common_params[\"messages\"][0][\"role\"] == \"system\":\n",
    "            common_params[\"messages\"][0][\"content\"] += f\" {json_instruction}\"\n",
    "        else:\n",
    "            common_params[\"messages\"].insert(0, {\"role\": \"system\", \"content\": json_instruction})\n",
    "\n",
    "    def _handle_response(self, response):\n",
    "        logger = logging.getLogger(\"handle_response\")\n",
    "        logger.info(\"Handling response\")\n",
    "        if self.stream:\n",
    "            logger.debug(\"Handling stream response\")\n",
    "            assistant_response = self._handle_stream(response, self.print_color, True)\n",
    "        else:\n",
    "            logger.debug(\"Handling non-stream response\")\n",
    "            assistant_response = self._handle_non_stream(response)\n",
    "\n",
    "        logger.debug(f\"Raw assistant response: {assistant_response}\")\n",
    "\n",
    "        if assistant_response is None:\n",
    "            raise ValueError(\"Received None response from assistant\")\n",
    "\n",
    "        if self.json_mode:\n",
    "            self.logger.debug(\"JSON mode is enabled, attempting to parse response\")\n",
    "            if self.provider == \"openai\":\n",
    "                try:\n",
    "                    assistant_response = json.loads(assistant_response)\n",
    "                except json.JSONDecodeError as json_error:\n",
    "                    self.logger.error(f\"OpenAI JSON decoding error: {json_error}\")\n",
    "                    raise\n",
    "            else:\n",
    "                try:\n",
    "                    assistant_response = safe_json_loads(assistant_response, error_prefix=\"Intelisys JSON parsing: \")\n",
    "                except Exception as json_error:\n",
    "                    self.logger.error(f\"safe_json_loads error: {json_error}\")\n",
    "                    raise\n",
    "\n",
    "        if self.provider == \"openai\" and self.output_model:\n",
    "            function_call = response.choices[0].message.function_call\n",
    "            if function_call and function_call.name == \"output\":\n",
    "                try:\n",
    "                    self.structured_output = self.output_model.model_validate_json(function_call.arguments)\n",
    "                except ValidationError:\n",
    "                    self.logger.warning(\"Failed to validate structured output\")\n",
    "                    self.structured_output = None\n",
    "            else:\n",
    "                self.structured_output = None\n",
    "\n",
    "        self.logger.debug(f\"Final processed assistant response: {assistant_response}\")\n",
    "        self.add_message(\"assistant\", str(assistant_response))\n",
    "        self.trim_history()\n",
    "        return assistant_response\n",
    "\n",
    "    def _handle_stream(self, response, color, should_print):\n",
    "        self.logger.debug(\"Handling stream response\")\n",
    "        assistant_response = \"\"\n",
    "        for chunk in response:\n",
    "            content = self._extract_content(chunk)\n",
    "            if content:\n",
    "                if should_print:\n",
    "                    print(colored(content, color), end=\"\", flush=True)\n",
    "                assistant_response += content\n",
    "        print()\n",
    "        return assistant_response\n",
    "\n",
    "    def _handle_non_stream(self, response):\n",
    "        self.logger.debug(\"Handling non-stream response\")\n",
    "        if self.provider == \"anthropic\":\n",
    "            return response.content[0].text\n",
    "        elif self.provider == \"openai\":\n",
    "            message = response.choices[0].message\n",
    "            if message.function_call:\n",
    "                return message.function_call.arguments\n",
    "            else:\n",
    "                return message.content\n",
    "        else:\n",
    "            return response.choices[0].message.content\n",
    "\n",
    "    def _extract_content(self, chunk):\n",
    "        if self.provider == \"anthropic\":\n",
    "            return chunk.delta.text if chunk.type == 'content_block_delta' else None\n",
    "        return chunk.choices[0].delta.content if chunk.choices[0].delta.content else None\n",
    "\n",
    "    def trim_history(self):\n",
    "        if self.max_history_words > 0:\n",
    "            self.logger.info(\"Trimming history\")\n",
    "            words_count = sum(len(str(m[\"content\"]).split()) for m in self.history if m[\"role\"] != \"system\")\n",
    "            while words_count > self.max_history_words and len(self.history) > 1:\n",
    "                removed_message = self.history.pop(0)\n",
    "                words_count -= len(str(removed_message[\"content\"]).split())\n",
    "            self.logger.debug(f\"History trimmed. Current word count: {words_count}\")\n",
    "        else:\n",
    "            self.history.clear()\n",
    "            self.logger.debug(\"History cleared (max_history_words is 0)\")\n",
    "        return self\n",
    "\n",
    "    def add_message(self, role, content):\n",
    "        self.logger.debug(f\"Adding message with role: {role}\")\n",
    "        self.logger.debug(f\"Message content: {content[:50]}...\")  # Log first 50 chars\n",
    "        if role == \"user\" and self.max_words_per_message:\n",
    "            if isinstance(content, str):\n",
    "                content += f\" please use {self.max_words_per_message} words or less\"\n",
    "            elif isinstance(content, list) and content and isinstance(content[0], dict) and content[0].get('type') == 'text':\n",
    "                content[0]['text'] += f\" please use {self.max_words_per_message} words or less\"\n",
    "\n",
    "        if self.max_history_words > 0:\n",
    "            self.history.append({\"role\": role, \"content\": content})\n",
    "            self.trim_history()\n",
    "        return self\n",
    "\n",
    "    def set_default_template(self, template: str) -> 'Intelisys':\n",
    "        self.logger.debug(\"*Set Default template*\")\n",
    "        self.default_template = template\n",
    "        return self\n",
    "\n",
    "    def set_default_persona(self, persona: str) -> 'Intelisys':\n",
    "        self.logger.debug(\"*Setting default persona*\")\n",
    "        self.default_persona = persona\n",
    "        return self\n",
    "\n",
    "    def set_template_instruction(self, set: str, instruction: str):\n",
    "        self.logger.debug(f\"Setting template instruction: set={set}, instruction={instruction}\")\n",
    "        self.template_instruction = self._go_get_api(set, instruction, \"Promptsys\")\n",
    "        return self\n",
    "\n",
    "    def set_template_persona(self, persona: str):\n",
    "        self.logger.debug(f\"Setting template persona: {persona}\")\n",
    "        self.template_persona = self._go_get_api(\"persona\", persona, \"Promptsys\")\n",
    "        return self\n",
    "\n",
    "    def set_template_data(self, render_data: Dict):\n",
    "        self.logger.debug(\"Setting template data\")\n",
    "        self.template_data = render_data\n",
    "        return self\n",
    "\n",
    "    def template_chat(self, \n",
    "                    render_data: Optional[Dict[str, Union[str, int, float]]] = None, \n",
    "                    template: Optional[str] = None, \n",
    "                    persona: Optional[str] = None) -> Union[str, BaseModel]:\n",
    "        \"\"\"\n",
    "        Send a chat message using a template and get the AI's response.\n",
    "\n",
    "        Args:\n",
    "            render_data (dict, optional): Data to render the template with.\n",
    "            template (str, optional): The template string to use. If None, uses the default template.\n",
    "            persona (str, optional): The persona to use for the system message. If None, uses the default persona.\n",
    "\n",
    "        Returns:\n",
    "            Union[str, BaseModel]: The AI's response as a string or a Pydantic model instance if structured output is used.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If there's an error rendering the template.\n",
    "\n",
    "        Usage:\n",
    "            response = intelisys.template_chat(\n",
    "                render_data={\"name\": \"Alice\", \"question\": \"What's the weather like?\"},\n",
    "                template=\"Hello {{name}}, {{question}}\",\n",
    "                persona=\"You are a weather expert.\"\n",
    "            )\n",
    "        \"\"\"\n",
    "        self.logger.info(\"*Template*\")\n",
    "        try:\n",
    "            template = Template(template or self.default_template)\n",
    "            merged_data = {**self.template_data, **(render_data or {})}\n",
    "            prompt = template.render(**merged_data)\n",
    "            self.logger.debug(f\"Rendered prompt: {prompt[:100]}...\")  # Log first 100 chars\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error rendering template: {e}\")\n",
    "            raise ValueError(f\"Invalid template: {e}\")\n",
    "\n",
    "        self.set_system_message(persona or self.default_persona)\n",
    "        return self.chat(prompt)\n",
    "\n",
    "    def transcript(self, audio_file_path: str, model: str = \"whisper-1\") -> str:\n",
    "        \"\"\"\n",
    "        Transcribe an audio file using OpenAI's Whisper model.\n",
    "\n",
    "        Args:\n",
    "            audio_file_path (str): Path to the audio file to transcribe.\n",
    "            model (str, optional): The model to use for transcription. Defaults to \"whisper-1\".\n",
    "\n",
    "        Returns:\n",
    "            str: The transcribed text.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the provider is not OpenAI or if the audio file is not found.\n",
    "\n",
    "        Usage:\n",
    "            transcription = intelisys.transcript(\"/path/to/audio.mp3\")\n",
    "        \"\"\"\n",
    "        self.logger.debug(f\"Transcribing audio file: {audio_file_path}\")\n",
    "\n",
    "        if self.provider != \"openai\":\n",
    "            raise ValueError(\"The transcript method is only supported for the OpenAI provider.\")\n",
    "\n",
    "        if not os.path.exists(audio_file_path):\n",
    "            raise FileNotFoundError(f\"Audio file not found: {audio_file_path}\")\n",
    "\n",
    "        try:\n",
    "            with open(audio_file_path, \"rb\") as audio_file:\n",
    "                transcript = self.client.audio.transcriptions.create(\n",
    "                    model=model,\n",
    "                    file=audio_file\n",
    "                )\n",
    "            \n",
    "            self.logger.debug(\"Transcription completed successfully\")\n",
    "            return transcript.text\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error during transcription: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    @contextmanager\n",
    "    def template_context(self, template: Optional[str] = None, persona: Optional[str] = None):\n",
    "        self.logger.debug(\"*Template context*\")\n",
    "        old_template, old_persona = self.default_template, self.default_persona\n",
    "        if template:\n",
    "            self.set_default_template(template)\n",
    "        if persona:\n",
    "            self.set_default_persona(persona)\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            self.default_template, self.default_persona = old_template, old_persona\n",
    "            self.logger.debug(\"Exiting template context\")\n",
    "\n",
    "    # Async methods\n",
    "    async def chat_async(self, user_input, **kwargs):\n",
    "        self.logger.debug(\"Async chat method called\")\n",
    "        await self.add_message_async(\"user\", user_input)\n",
    "        self.last_response = await self.get_response_async(**kwargs)\n",
    "        return self.last_response\n",
    "\n",
    "    async def add_message_async(self, role, content):\n",
    "        self.logger.debug(f\"Async adding message with role: {role}\")\n",
    "        self.add_message(role, content)\n",
    "        return self\n",
    "\n",
    "    async def set_system_message_async(self, message=None):\n",
    "        self.logger.debug(\"Async setting system message\")\n",
    "        self.set_system_message(message)\n",
    "        return self\n",
    "\n",
    "    async def get_response_async(self, color=None, should_print=True, **kwargs):\n",
    "        self.logger.debug(\"Async get_response method called\")\n",
    "        color = color or self.print_color\n",
    "        max_tokens = kwargs.pop('max_tokens', 4000 if self.provider != \"anthropic\" else 8192)\n",
    "\n",
    "        response = await self._create_response_async(max_tokens, **kwargs)\n",
    "\n",
    "        assistant_response = await self._handle_stream_async(response, color, should_print) if self.stream else await self._handle_non_stream_async(response)\n",
    "\n",
    "        if self.json_mode and self.provider == \"openai\":\n",
    "            try:\n",
    "                assistant_response = json.loads(assistant_response)\n",
    "            except json.JSONDecodeError as json_error:\n",
    "                self.logger.error(f\"JSON decoding error: {json_error}\")\n",
    "                raise\n",
    "\n",
    "        await self.add_message_async(\"assistant\", str(assistant_response))\n",
    "        await self.trim_history_async()\n",
    "        return assistant_response\n",
    "\n",
    "    async def _create_response_async(self, max_tokens, **kwargs):\n",
    "        self.logger.debug(f\"Creating async response with max_tokens={max_tokens}\")\n",
    "        if self.provider == \"anthropic\":\n",
    "            return await self.client.messages.create(\n",
    "                model=self.model,\n",
    "                system=self.system_message,\n",
    "                messages=self.history,\n",
    "                stream=self.stream,\n",
    "                temperature=self.temperature,\n",
    "                max_tokens=max_tokens,\n",
    "                extra_headers={\"anthropic-beta\": \"max-tokens-3-5-sonnet-2024-07-15\"},\n",
    "                **kwargs\n",
    "            )\n",
    "        else:\n",
    "            common_params = {\n",
    "                \"model\": self.model,\n",
    "                \"messages\": [{\"role\": \"system\", \"content\": self.system_message}] + self.history,\n",
    "                \"stream\": self.stream,\n",
    "                \"temperature\": self.temperature,\n",
    "                \"max_tokens\": max_tokens,\n",
    "                **kwargs\n",
    "            }\n",
    "            if self.json_mode and self.provider == \"openai\":\n",
    "                common_params[\"response_format\"] = {\"type\": \"json_object\"}\n",
    "            \n",
    "            if self.provider == \"openai\" and self.output_model:\n",
    "                common_params[\"response_format\"] = {\"type\": \"json_object\"}\n",
    "                common_params[\"functions\"] = [{\n",
    "                    \"name\": \"output\",\n",
    "                    \"parameters\": self.output_model.model_json_schema()\n",
    "                }]\n",
    "                common_params[\"function_call\"] = {\"name\": \"output\"}\n",
    "\n",
    "            return await self.client.chat.completions.create(**common_params)\n",
    "\n",
    "    async def _handle_stream_async(self, response, color, should_print):\n",
    "        self.logger.debug(\"Handling async stream response\")\n",
    "        assistant_response = \"\"\n",
    "        async for chunk in response:\n",
    "            content = self._extract_content_async(chunk)\n",
    "            if content:\n",
    "                if should_print:\n",
    "                    print(colored(content, color), end=\"\", flush=True)\n",
    "                assistant_response += content\n",
    "        print()\n",
    "        return assistant_response\n",
    "\n",
    "    async def _handle_non_stream_async(self, response):\n",
    "        self.logger.debug(\"Handling async non-stream response\")\n",
    "        return response.content[0].text if self.provider == \"anthropic\" else response.choices[0].message.content\n",
    "\n",
    "    def _extract_content_async(self, chunk):\n",
    "        if self.provider == \"anthropic\":\n",
    "            return chunk.delta.text if chunk.type == 'content_block_delta' else None\n",
    "        return chunk.choices[0].delta.content if chunk.choices[0].delta.content else None\n",
    "\n",
    "    async def trim_history_async(self):\n",
    "        self.logger.debug(\"Async trimming history\")\n",
    "        self.trim_history()\n",
    "        return self\n",
    "\n",
    "    def set_output_model(self, model: Type[BaseModel]):\n",
    "        \"\"\"\n",
    "        Set the Pydantic model for structured output (OpenAI provider only).\n",
    "\n",
    "        Args:\n",
    "            model (Type[BaseModel]): The Pydantic model defining the structure of the output.\n",
    "\n",
    "        Returns:\n",
    "            self: The Intelisys instance for method chaining.\n",
    "        \"\"\"\n",
    "        if self.provider != \"openai\":\n",
    "            self.logger.warning(\"Structured output is only supported for the OpenAI provider. This setting will be ignored.\")\n",
    "        else:\n",
    "            self.output_model = model\n",
    "        return self\n",
    "\n",
    "    def results(self) -> Union[str, BaseModel, None]:\n",
    "        \"\"\"\n",
    "        Get the results of the last chat operation.\n",
    "\n",
    "        Returns:\n",
    "            Union[str, BaseModel, None]: The chat response as a string, \n",
    "            a Pydantic model instance (if structured output is used with OpenAI), \n",
    "            or None if not available.\n",
    "        \"\"\"\n",
    "        if self.provider == \"openai\" and self.structured_output:\n",
    "            return self.structured_output\n",
    "        return self.last_response\n",
    "\n",
    "    async def template_chat_async(self, \n",
    "                                render_data: Optional[Dict[str, Union[str, int, float]]] = None, \n",
    "                                template: Optional[str] = None, \n",
    "                                persona: Optional[str] = None, \n",
    "                                parse_json: bool = False) -> 'Intelisys':\n",
    "        \"\"\"\n",
    "        Asynchronously send a chat message using a template and get the AI's response.\n",
    "\n",
    "        Args:\n",
    "            render_data (dict, optional): Data to render the template with.\n",
    "            template (str, optional): The template string to use. If None, uses the default template.\n",
    "            persona (str, optional): The persona to use for the system message. If None, uses the default persona.\n",
    "\n",
    "        Returns:\n",
    "            Intelisys: The Intelisys instance for method chaining.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If there's an error rendering the template or processing the response.\n",
    "\n",
    "        Usage:\n",
    "            response = await intelisys.template_chat_async(\n",
    "                render_data={\"name\": \"Bob\", \"question\": \"What's the capital of France?\"},\n",
    "                template=\"Hello {{name}}, {{question}}\",\n",
    "                persona=\"You are a geography expert.\"\n",
    "            )\n",
    "            result = intelisys.last_response\n",
    "        \"\"\"\n",
    "        self.logger.debug(\"Async template chat method called\")\n",
    "        try:\n",
    "            template = Template(template or self.default_template)\n",
    "            merged_data = {**self.template_data, **(render_data or {})}\n",
    "            prompt = template.render(**merged_data)\n",
    "            self.logger.debug(f\"Rendered prompt: {prompt[:50]}...\")  # Log first 50 chars\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error rendering template: {e}\")\n",
    "            raise ValueError(f\"Invalid template: {e}\")\n",
    "\n",
    "        await self.set_system_message_async(persona or self.default_persona)\n",
    "        response = await self.chat_async(prompt)\n",
    "        \n",
    "        if self.json_mode:\n",
    "            if isinstance(response, dict):\n",
    "                self.last_response = response\n",
    "            elif isinstance(response, str):\n",
    "                try:\n",
    "                    self.last_response = json.loads(response)\n",
    "                except json.JSONDecodeError:\n",
    "                    self.last_response = safe_json_loads(response, error_prefix=\"Intelisys async template chat JSON parsing: \")\n",
    "            else:\n",
    "                self.logger.error(f\"Unexpected response type: {type(response)}\")\n",
    "                raise ValueError(f\"Unexpected response type: {type(response)}\")\n",
    "        else:\n",
    "            self.last_response = response\n",
    "        \n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "intelisys = Intelisys(provider=\"openai\")\n",
    "transcription = intelisys.transcript(\"/Users/lifsys/Documents/devhub/lib/intelisys/testing.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = intelisys.chat(f\"Summarize this transcription: {transcription}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! As an AI language model, I don't have feelings, but I'm functioning well and ready to assist you. How can I help you today? Do you have any questions or topics you'd like to discuss?\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Synchronous usage\n",
    "intelisys = Intelisys()\n",
    "response = intelisys.chat(\"Hello, how are you?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'greeting': 'Hello!',\n",
       " 'description': \"As an AI language model, I don't have feelings, but I'm functioning well and ready to assist you.\",\n",
       " 'offer': 'How can I help you today?',\n",
       " 'invitation': \"Do you have any questions or topics you'd like to discuss?\"}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "safe_json_loads(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Resume lacks specific army rank; describes leadership roles in military service.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Template usage\n",
    "response = intelisys.template_chat(\n",
    "    template=\"Summarize this in {{words}} words: {{text}}\",\n",
    "    render_data={\"words\": 10, \"text\": \"After carefully reviewing the provided resume, I don't see any explicit mention of a specific army rank. The document describes various military and civilian roles and responsibilities, but does not state a particular rank like Lieutenant, Captain, Major, etc. \\nThe experience described suggests the individual has had significant leadership roles in the U.S. Army and Army Reserve, including positions like: Chief of the Central Team in the Army Reserve Sustainment Command Transportation Officer in the US Army Reserve Deployment Support Command Director of Logistics for Area Support Group - Kuwait Contingency Contracting Officer in Southwest Asia. While these roles imply a relatively senior position, without an explicitly stated rank, I cannot confirm any specific army rank from the information provided. The focus seems to be more on describing job responsibilities and accomplishments rather than military rank progression.\"}\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-24 21:27:50 | INFO | *Template*\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Alice', 'greeting': 'Hello!', 'mood': \"I'm doing well, thank you for asking.\", 'current_status': 'Ready to assist', 'timestamp': '2023-05-09T12:34:56Z'}\n"
     ]
    }
   ],
   "source": [
    "intelisys = Intelisys(json_mode=True, log=\"INFO\")\n",
    "\n",
    "# Set a default template\n",
    "intelisys.set_default_template(\"Return JSON, {{ name }}! {{ question }}\")\n",
    "\n",
    "# Use the template in a chat\n",
    "response = intelisys.template_chat(\n",
    "    render_data={\"name\": \"Alice\", \"question\": \"How are you today?\"}\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Develop test cases\\n\\nEngineer preliminary prompt\\n\\nTest prompt against cases\\n\\nRefine prompt\\n\\nTest against held-out evals\\n\\nShip polished prompt\\n\\nDon't forget edge cases!\\n\\nEVALS!\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Image OCR example\n",
    "intelisys = Intelisys(provider=\"openrouter\", model=\"google/gemini-pro-vision\")  # Make sure to use a model that supports image processing\n",
    "result = (intelisys\n",
    "    .image(\"https://mintlify.s3-us-west-1.amazonaws.com/anthropic/images/how-to-prompt-eng.png\")\n",
    "    .chat(\"Please provide the complete text in the following image(s).\")\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List, Optional\n",
    "\n",
    "class MovieReview(BaseModel):\n",
    "    title: str\n",
    "    year: int\n",
    "    director: str\n",
    "    genre: List[str]\n",
    "    rating: float\n",
    "    summary: str\n",
    "    pros: List[str]\n",
    "    cons: List[str]\n",
    "\n",
    "# Create an Intelisys instance\n",
    "intelisys = Intelisys(provider=\"openai\", model=\"gpt-4o-mini\")\n",
    "\n",
    "# Use method chaining to set up and execute the request\n",
    "try:\n",
    "    result = (intelisys\n",
    "        .set_output_model(MovieReview)\n",
    "        .set_system_message(\"You are a professional film critic with extensive knowledge of cinema history\")\n",
    "        .image(\"https://intheposter.com/cdn/shop/files/the-manager-in-the-poster-1_5000x.jpg?v=1694762527\")\n",
    "        .chat(\"Analyze this movie poster and provide a detailed review of the film. Include information about the movie, your rating, and a brief summary of your thoughts.\")\n",
    "    )\n",
    "\n",
    "    # Parse the JSON string into a Python dictionary\n",
    "    result = safe_json_loads(result)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'The Manager',\n",
       " 'year': 2023,\n",
       " 'director': 'Chris Nolan',\n",
       " 'genre': ['Drama', 'Thriller'],\n",
       " 'rating': 8.5,\n",
       " 'summary': \"'The Manager' is a gripping exploration of ambition and moral ambiguity in the corporate world. Jamie Nolan delivers a standout performance as a young executive navigating the treacherous waters of corporate politics, where every decision could lead to success or ruin. The film's stylish direction and sharp dialogue keep the audience engaged, while its underlying themes of power and ethics resonate deeply. The cinematography is striking, with a bold color palette that enhances the film's tension and urgency. Overall, 'The Manager' is a thought-provoking and visually stunning film that leaves a lasting impression.\",\n",
       " 'pros': ['Strong performance by Jamie Nolan',\n",
       "  'Stylish direction and cinematography',\n",
       "  'Engaging and thought-provoking themes',\n",
       "  'Sharp dialogue and pacing'],\n",
       " 'cons': ['Some plot points may feel predictable',\n",
       "  'Character development could be deeper']}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
